{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Configuración de Ollama y Jupyter Lab - Primera consulta a un modelo - JKGzenna\n",
    "\n",
    "Ejemplo para resumir una página web y utilizar un modelo de código abierto con Ollama que se ejecute localmente a través de Ollama API en lugar de OpenAI, Anthropic o cualquier otra plataforma de pago\n",
    "\n",
    "Se puede utilizar esta técnica para todos los proyectos posteriores si se prefiere no utilizar API de pago (closed source).\n",
    "\n",
    "**Beneficios:**\n",
    "1. Sin cargos por API: código abierto\n",
    "2. Los datos no salen de su ordenador\n",
    "\n",
    "**Desventajas:**\n",
    "1. Tiene mucha menos potencia (parámetros) que el modelo Frontier de cualquier empresa de pago\n",
    "\n",
    "## Resumen de la instalación de Ollama\n",
    "\n",
    "¡Hemos levantado Ollama en stack de contendores junto con jupyter en un misma net de docker, y con retorno al host anfitrión, para que puedas trabajar con Ollama y Jupyter como si los tuvieras en local, pero estando contenerizados, de esa manera no 'ensuciamos' nuestros equipos instalando dependencias y programas de manera local, mucho mejor en un contenedor al que le pasemos nuestros datos con un volumen.\n",
    "\n",
    "Una vez que hayas terminado, el servidor Ollama ya debería estar ejecutándose localmente en tu máquina, como si hubieras hecho la instalación en windows/linux/mac de Ollama.\n",
    "\n",
    "Si entras en: `http://localhost:11434/`\n",
    "\n",
    "Debería ver el mensaje: `Ollama is running`\n",
    "\n",
    "Cada vez que hagamos un uso de algun modelo de ollama que este pulleado en nuestro equipo automaticamente se levantara si esta pulleado, si no esta pulleado dará error, se debe hacer pull de el en terminal, al pasar un par de minutos sin consultas lo baja de nuevo, en ollama desde terminal podemos controlar los modelos igual que si fueran contenedores con los siguientes comandos:\n",
    "\n",
    "- `ollama pull`\n",
    "- `ollama list`\n",
    "- `ollama ps` (para ver modelos activos en ejecución)\n",
    "- `ollama rm <modelo>` (para eliminar modelos pulleados de nuestro equipo)\n",
    "- `ollama run <modelo>` (solo es necesario si queremos consumirlo desde terminal de manera explicita y hacer consultas desde allí, si no jupyter los levantará y bajará si están pulleados)\n",
    "- `ollama stop <modelo>` (pocas veces apagaremos a mano un modelo, ya que jupyter se encargará de bajarlos cuando no haya consultas, y si somos nosotros los que levantamos el modelo desde terminal, al terminar de hablar con el podemos salir y cerrarlo con `/bye`, en caso de que cerremos la ventana y el modelo se quede en ejecución, por eso o cualquier otro motivo, tendremos que bajarlo con este comando)\n",
    "\n",
    "Si se ha realizado instalación con contenedores de docker se puede ver también en esta URL [http://host.docker.internal:11434/](http://host.docker.internal:11434/), a la cual es necesario apuntar desde Jupyter cuando llamemos al API de ollama en caso de estar ejecutando Ollama en contenedor como podemos ver en esta issue de GitHub: https://github.com/ollama/ollama/issues/3200\n",
    "\n",
    "Después de levantar este stack con el `docker-compose.yml` debes abrir un terminal aquí en Jupyter haciendo click en `File` y después en `New Console For Notebook` a ejecutar el siguiente comando para instalar las dependencias necesarias, si necesitas alguna más sólo debes ejecutar el comando `!pip install` o `pip install`, al ejecutar este comando nos avisará de que es más recomendable hacerlo en un entorno virtualizado con python (pipenv), pero no vamos a hacerlo así porque no estamos en el sistema de la máquina anfitriona, donde sin duda prepararíamos un entorno virtualizado clásico con pipenv,  o bien con anaconda, pero todo eso \"ensucia\" nuestro sistema local anfitrión, pero como estamos dentro de un contenedor de docker creado específicamente para esta tarea, realizaremos instalación global y como root de las dependencias, sin utilizar un entorno virtualizado (pipenv o conda), ya que este contenedor es específico para esta tarea de Jupyter, pudiendo destruirlo cuando deseemos, ya que los datos con los que trabjaremos se almacenan de manera segura en un volumen de datos de docker conectado al sistema local anfitrión.\n",
    "\n",
    "`!pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios para la prueba del API de Ollama\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "\n",
    "# Si hemos levantado Ollama en un contendor de Docker, debemos hacer la llamada a 'host.docker.internal' u 'ollama',\n",
    "# en lugar de a 'localhost', de lo contrario obtendremos error del API al estar trabajando contra 'localhost'\n",
    "\n",
    "# Utilizando las DNS que nos crea Docker al instalarse en el '../../../etc/hosts', esta dirección funciona en el navegador\n",
    "#OLLAMA_API = \"http://host.docker.internal:11434/api/chat\"\n",
    "\n",
    "# # Utilizando el nombre del servicio que hemos creado en Docker para la imagen de Ollama, esta dirección no funciona en el navegador, solo con el API de Ollama,\n",
    "# ya que no es una ruta expuesta, sino un servicio interno de docker que Jupyter es capaz de ver porque está en el mismo stack y red de Docker que Ollama\n",
    "OLLAMA_API = \"http://ollama:11434/api/chat\"\n",
    "\n",
    "# Incluimos la variable cabeceras para un mejor manejo de la solicitud y no sobrecargar el post que vamos a realizar más adelante\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Elegimos un modelo opensource de Ollama'\n",
    "MODEL = \"gemma3n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una lista de mensajes utilizando el mismo formato que usamos para OpenAI, Anthropic o cualquier otro modelo Frontier\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Dame la receta de la tortilla de patata\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos el objeto payload para pasarle directamente el nombre 'payload' y no todo lo que contiene en el siguiente post que vamos a realizar, así no sobrecargamos dicho post\n",
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos como parametros del request el API de Ollama, la varibale con las cabeceras y el objeto con el payload que hemos preparado en un post que nos devolvera la respuesta\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "# Respuesta RAW sin formatear\n",
    "# Respuesta en json normal\n",
    "#print(response.json()['message']['content'])\n",
    "# Respuesta formateada en Markdown\n",
    "reply = (response.json()['message']['content'])\n",
    "display(Markdown(reply))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
