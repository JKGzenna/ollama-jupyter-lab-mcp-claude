{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346c2913-14e5-4795-b544-29c362988fb2",
   "metadata": {},
   "source": [
    "## API Ollama Documentation: `https://github.com/ollama/ollama/blob/main/docs/api.md`\n",
    "\n",
    "- #### En este notebook podemos ver un ejemplo de como hacer pull de un modelo opensource con una simple llamada al API de Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bc2ce-25ed-4bd7-8631-d30a5b452d39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## JKGzenna ##\n",
    "## Documentación del API de Ollama -->  https://github.com/ollama/ollama/blob/main/docs/api.md ##\n",
    "## Podemos preparar todo en bloques más pequeños o utilizar un único bloque en el que codificar todo, en este caso de uso prepararemos todo en un bloque ##\n",
    "\n",
    "## Imports necesarios ##\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "## Definimos nuestras constantes ##\n",
    "# Utilizando las DNS que nos crea Docker al instalarse en el '../etc/hosts', esta dirección funciona en el navegador\n",
    "#OLLAMA_API_PULL  = \"http://http://host.docker.internal:11434/api/pull\"\n",
    "#OLLAMA_API_GENERATE = \"http://http://host.docker.internal:11434/api/generate\"\n",
    "# # Utilizando el nombre del servicio que hemos creado en Docker para la imagen de Ollama, esta dirección no funciona en el navegador, solo con el API de Ollama,\n",
    "# ya que no es una ruta expuesta, sino un servicio interno de docker que Jupyter es capaz de ver porque está en el mismo stack y red de Docker que Ollama\n",
    "OLLAMA_API_PULL  = \"http://ollama:11434/api/pull\"\n",
    "OLLAMA_API_GENERATE = \"http://ollama:11434/api/generate\"\n",
    "\n",
    "\n",
    "## Definimos nuestras variables ##\n",
    "## Nombre del modelo del que se hará run, si no lo tenemos, se hará pull de él y se levantará, (si no lo tenemos puede tardar en hacer el pull) ##\n",
    "model_name = \"gemma3\"\n",
    "## Preparamos el prompt de nuestra consulta dentro de la varibale prompt ##\n",
    "## Escribe tu pregunta de manera natural ##\n",
    "prompt = \"Me puedes explicar la crisis financiera del 2008?\"\n",
    "\n",
    "\n",
    "## Pull de un modelo opensource de 'https://ollama.com/search', utilizando las variables y constantes preparadas para llamar al API de Ollama (sin ir al terminal)\n",
    "_ = requests.post(OLLAMA_API_PULL, json={\n",
    "    \"name\": model_name\n",
    "})\n",
    "\n",
    "\n",
    "## Post al API de Ollama con todas las variables y constantes preparadas para obtener la respuesta ##\n",
    "response = requests.post(OLLAMA_API_GENERATE, json={\n",
    "    \"model\": model_name,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": False\n",
    "}).json()\n",
    "\n",
    "# Respuesta RAW sin formatear\n",
    "#response[\"response\"]\n",
    "\n",
    "# Respuesta en Markdown sin utilizar una variable para incluir el response\n",
    "#display(Markdown((response[\"response\"])))\n",
    "\n",
    "# Incluimos el response en la variable 'reply' y la mostramos en Markdown\n",
    "reply = (response[\"response\"])\n",
    "display(Markdown(reply))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
